<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.png" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.png" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Yan Wang (王龑)</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
<link rel="stylesheet" href="./stylesheets/styles.css">
<link rel="stylesheet" href="./stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./javascripts/analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<!-- <h7>Yan Wang </h7><br><br>-->
<div>
<img src="./sub_img/zjz_wy_2504.jpg" border="0" width="90%"><br></div><br>


<p>
<!-- <small>📍School of Data Science of Engineering (DaSE) </small><br>
<small>📍East China Normal University (ECNU)</small><br> -->
<small>📍Room X101, Shuxueguan</small><br>
<small>📧yanwang[at]dase.ecnu.edu.cn</small><br>
<small>📧yanwang19[at]fudan.edu.cn</small><br><br>
<a href="https://github.com/wangyanckxx/" target="_blank">[GitHub]</a>
<a href="https://dblp.org/pid/59/2227-68.html" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://wangyanckxx.github.io/">Homepage</a></p>
<!--<p class="view"><a href="sub_publication.html">Publications</a></p>-->
<p class="view"><a href="" target="_blank">Publications</a></p>
<p class="view"><a href="datasets.html" target="_blank">Datasets</a></p>
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>


<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a> <p style="font-size: 28px;">Yan Wang (王 龑)</p> </h2>
<!-- 	<p style="font-size: 12px;">Research Professor (Zijiang Young Scholar)</p>
	<p style="font-size: 12px;">School of Data Science of Engineering (DaSE),</p>
	<p style="font-size: 12px;">East China Normal University (ECNU), Shanghai, China</p> -->

	<strong> 青年研究员（“双百人才计划”紫江青年学者），博士生导师</strong> <br>
	华东师范大学数据科学与工程学院<br>
	<strong> Research Professor (Zijiang Young Scholar) </strong> <br>
	School of Data Science of Engineering (DaSE), <br>
	East China Normal University (ECNU), Shanghai, China <br>
<!-- 		<p style="font-size: 20px;"></p>
	<p style="font-size: 20px;">School of Data Science and Engineering,</p>
	<p style="font-size: 20px;">East China Normal University (ECNU), Shanghai, China</p> -->

<div style="text-align: justify; display: block; margin-right: auto;">
<p>

	<br>

Dr. Yan Wang is the <a href="https://faculty.ecnu.edu.cn/_s37/wy2/main.psp" target="_blank" >Research Professor (Zijiang Young Scholar)</a> with School of Data Science and Engineering, East China Normal University in April 2025. He received the Ph.D. degree in Computer Science and Technology from Fudan University in Jan. 2023, and then then continued to work as a postdoctoral fellow at the Fudan University, both with <a href="http://faet.fudan.edu.cn/e4/28/c23898a255016/page.htm" target="_blank" >Prof. Wenqiang Zhang</a> as the advisor. In the past 3 years, he has published 20+ papers <a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a> as the first or corresponding author (co-author) in top conferences and journals such as CVPR, ICCV, ECCV, NeurIPS, AAAI, ACM MM, Information Fusion, and IEEE TII. He won the <a href="https://www.sciencedirect.com/journal/information-fusion/about/awards/2024-inffus-best-paper-best-survey-and-best-editor-award" target="_blank">2024 Information Fusion Best Survey Award</a> (the only best review paper award in 2024, the only first author), Shanghai Super Postdoctoral Fellow, and the <a href="https://news.fudan.edu.cn/2025/0107/c31a143936/page.htm">2024 Fudan University "Top Ten Scientific and Technological Advances" Award </a> (Guanghua No. 1 Humanoid Robot). His research interests include expression recognition and affective computing, embodied intelligence, ‌AIGC, multimedia information processing, machine vision and enhancement.

	    <br>


	
	
</ul>
<br>
<hr />

<h2>

<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>News and Olds</h2>

<ul>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2025-04]</span>  I am the Research Professor, Zijiang Young Scholar (equivalent to Pre-tenure Professor) with School of Data Science and Engineering, East China Normal University, China. </li>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2025-03]</span>  One paper has been accepted by <strong>Information Fusion 2025 (IF=14.8) </strong></li>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2025-02]</span>  One paper has been accepted by <strong>CVPR 2025</strong></li>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58); ">[2025-01]</span>  One paper has been accepted by <strong>ICRA 2025</strong></li>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2025-01]</span> <strong> <font color="#ff0000">Congratulations.</font> </strong> "Development and Industrialization of Embodied Universal Humanoid Robot (Fudan Guanghua No. 1) " was selected as <strong> <font color="#ff0000">"Top Ten Scientific and Technological Advances" </font> </strong> of Fudan University in 2024 </li>

 <li> <span style="font-weight: 600; color: rgb(57, 45, 58); ">[2024-12]</span>  One paper has been accepted by <strong>AAAI 2025</strong></li>

<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-11]</span> <strong> <font color="#ff0000">Congratulations.</font> </strong> "A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances" was selected as <strong> <font color="#ff0000">Information Fusion 2024 Best Survey Award</font> </strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-10]</span>  A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection can be viewed from <strong> <a href="https://arxiv.org/abs/2410.21982" target="_blank">[Arxiv]</a></strong>  <strong> <a href="https://github.com/Sunny5250/Awesome-Multi-Setting-UIAD" target="_blank">[Github]</a></strong></li>

 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-09]</span>  One paper has been accepted by <strong>NeurIPS 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-08]</span>  A Survey on Facial Expression Recognition of Static and Dynamic Emotions can be viewed from <strong> <a href="https://arxiv.org/pdf/2408.15777" target="_blank">[Arxiv]</a></strong>  <strong> <a href="https://github.com/wangyanckxx/SurveyFER" target="_blank">[Github]</a></strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-08]</span>  One paper has been accepted by <strong>Information Sciences</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-07]</span>  Two papers ( <strong> <font color="#ff0000">One Oral</font> </strong> ) have been accepted by <strong>ACMMM 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-17]</span>  One paper has been accepted by <strong>ECCV 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-04]</span>  One paper has been accepted by <strong>IJCAI 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-03]</span>  Two papers have been accepted by <strong>CVPR 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-02]</span>  We will release a Multi-Scene Unsupervised Anomaly Detection Dataset <strong> MSC-AD </strong> </li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-02]</span>  We will release the<strong> IndSR </strong> dataset</li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-01]</span>  One paper has been accepted by <strong>IEEE Trans. on Industrial Informatics (IF: 12.3)</strong></li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  One paper has been accepted by <strong>AAAI 2024</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  Two papers have been accepted by <strong>Computer Communications (IF: 6.0)</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-11]</span>  One paper has been accepted by <strong>IEEE Trans. on Industrial Informatics (IF: 12.3)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-10]</span>  Three papers (<strong> <font color="#ff0000">Best Paper Nomination</font> </strong>) has been accepted by <strong>INSAI 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  Five papers ( <strong> <font color="#ff0000">Two Oral Papers</font> </strong> ) have been accepted by <strong>ACMMM 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-07]</span>  Two papers have been accepted by <strong>ICCV 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-03]</span>  One paper has been accepted by <strong>IEEE Trans. on CSVT (IF=8.4)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-06]</span>  Two paper have been accepted by <strong>ACMMM 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-05]</span>  We release the <strong> FERV39k</strong> dataset</li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-03]</span>  One paper has been accepted by <strong>Information Fusion (IF=18.6)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-02]</span>  One paper has been accepted by <strong>CVPR 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2021-11]</span>  One paper has been accepted by <strong>AAAI 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2020-09]</span>  One paper has been accepted by <strong>IEEE Trans. on Broadcasting</strong></li>


</ul>
<br>
<hr />

<!--<h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Low-Light Image Enhancement Platform: LLIE-Platform</h2>  -->
<!--  -->
<!--<ul>-->
<!--<li> Different deep models may be implemented in different platforms such as Caffe, Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different configurations, GPU versions, and hardware specifications. Such requirements are prohibitive to many researchers, especially for beginners who are new to this area and may not even have GPU resources.</font></a></li>-->
<!--<li> To resolve these problems, we develop an online platform, LLIE-Platform <a href="http://mc.nankai.edu.cn/ll" target="_blank"><font color="#ff0000">[LLIE-Platform]</font></a>. -->
<!--If you use this platform, please cite our paper "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", TPAMI, 2021.</li>-->
<!--</ul>-->
<!--<br>-->
<!--<hr />-->

<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Call for Papers:</h2>

<ul>
<li><font color="#000000"> Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis, IEEE Journal of Oceanic Engineering (IEEE-JOE) (SCI, IF: 3.554) <a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 30, 2021. <a href="https://ieeeoes.org/publications/ieee-journal-of-oceanic-engineering/joe-special-issues/" target="_blue"><font color="#ff0000">[Offical Link]</font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Depth-Related Processing and Applications in Visual Systems, Multimedia Tools and Applications (SCI, IF: 2.101) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 1, 2020. <a href="https://www.springer.com/journal/11042/updates/17918156" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications in Signal Processing : Image Communication (SCI, IF: 2.814) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: July 31, 2020. <a href="https://www.journals.elsevier.com/signal-processing-image-communication/call-for-papers/theories-algorithms-and-applications" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>[Closed]
<li><font color="#000000"> <strike>Special Session in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference(APSIPA ASC) 2019.</font><a href="http://www.apsipa2019.org/#" target="_blank"><font color="#ff0000"> [Link]</font>: Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#ff0000">[CFP]</strike></font></a></a></li>[Closed]

</ul>
<br>
<hr />
-->



<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprints/Manuscripts <a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a></h2>


     <br>


 <div class="publication">
            <img src="./logo/FER_Review.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">A Survey on Facial Expression Recognition of Static and Dynamic Emotions</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>

                <b>Yan Wang</b>, Shaoqi Yan, Yang Liu, Wei Song, Jing Liu, Yang Chang, Xinji Mai, Xiping Hu, Wenqiang Zhang<sup>✉</sup>, Zhongxue Gan<sup>✉</sup>. <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2408.15777">PDF</a>|
		            <a href="https://github.com/wangyanckxx/SurveyFER">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/SurveyFER">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />






 <div class="publication">
            <img src="./logo/Hi_EF.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Hi-EF: Benchmarking for Human-interaction-based Emotion Forecasting</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>
               Haoran Wang, Xinji Mai, Zeng Tao, Yan Wang, <b>Yan Wang<sup>✉</sup></b>, Jiawen Yu, Ziheng Zhou, Xuan Tong, Shaoqi Yan, Qing Zhao, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>. <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2407.16406">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



 <div class="publication">
            <img src="./logo/Align_DFER.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2403.04294">A3lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>
               Zeng Tao, <b>Yan Wang<sup>✉</sup></b>, Junxiong Lin, Haoran Wang, Xinji Mai,... , Wenqiang Zhang<sup>✉</sup> <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2403.04294">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



    <hr />

<div class="container">

    <h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Publications <a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a></h2>


<!--   < color="#ff0000"><strong>(* indicates equal contribution, and ✉ indicates corresponding author) </strong><-->
<!--<br>-->

<font color="#000000"><strong>(<sup>✉</sup> indicates corresponding author, and <sup>*</sup> indicates equal contribution) </strong></font>

    <br>
	<br />


<!--<b>ICCV, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading5" target="_blank"><font color="#ff0000">[Oral]</font></a></b>-->
<!--<hr />-->
    <br>
<!-- <h3> <font color="#930618">Intelligent Emotional Robots</font></h3>
 <br> -->




<div class="publication">
            <img src="./logo/Review_inforfusion.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances</a>
                </strong>
		  <br>
		<em><b>Information Fusion, 2022</b></em>  <strong> <font color="#ff0000">[ESI Highly Cited Paper]</font> </strong>
                <br>
               <b>Yan Wang</b>, Wei Song, Wei Tao, Dawei Yang, Antonio Liotta,Dawei Yang, Xinlei Li, Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2203.06935">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



<div class="publication">
            <img src="./logo/AD_Review.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection</a>
                </strong>
		  <br>
		<em><b>Information Fusion, 2025</b></em>
                <br>
                Yuxuan Lin,Yang Chang, Xuan Tong, Jiawen Yu, Antonio Liotta, Guofan Huang, Wei Song, Deyu Zeng, Zongze Wu, <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>. <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2410.21982">PDF</a>|
		            <a href="https://github.com/Sunny5250/Awesome-Multi-Setting-UIAD">Project Page</a>|
                    <a href="https://github.com/Sunny5250/Awesome-Multi-Setting-UIAD">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />

	<div class="publication">
            <img src="./logo/D2SP_cvpr2025.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition</a>
                </strong>
		  <br>
		<em><b>CVPR, 2025 </b></em>
                <br> Haoran Wang, Xinji Mai, Zeng Tao, Xuan Tong, Junxiong Lin,  <b>Yan Wang<sup>✉</sup></b>, Jiawen Yu,
Shaoqi Yan, Ziheng Zhou, Wenqiang Zhang<sup>✉</sup>.<br>

                <span class="links">
                    <a href="">PDF</a>
                    <a href="">Project Page</a>
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br />
     <br />
    <br />

<div class="publication">
            <img src="./logo/OUS_aaai2025.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">OUS: Bridging Scene Context and Facial Features to Overcome the Rigid Cognitive Problem</a>
                </strong>
		  <br>
		<em><b>AAAI, 2025 </b></em>
                <br> Xinji Mai, Haoran Wang, Zeng Tao, Junxiong Lin, Shaoqi Yan, <b>Yan Wang<sup>✉</sup></b>, Jiawen Yu, Xuan Tong, Yating Li, Wenqiang Zhang<sup>✉</sup>.<br>

                <span class="links">
                    <a href="https://arxiv.org/pdf/2405.18769">PDF</a>
                    <a href="">Project Page</a>
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br />
     <br />
    <br />


    <div class="publication">
            <img src="./logo/ZhongjingGPT.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">ZhongJingGPT: An Expert Knowledge-Guided Language Model for Traditional Chinese Medicine</a>
                </strong>
		  <br>
		<em><b>Tsinghua Science and Technology, 2025 </b></em>
                <br> Yanlan Kang, Yang Chang, ... ,<b>Yan Wang<sup>✉</sup></b>, Haofen Wang<sup>✉</sup> , William Cheng-Chung Chu<sup>✉</sup> , Wenqiang Zhang<sup>✉</sup>.<br>

                <span class="links">
                    <a href="">PDF</a>
                    <a href="">Project Page</a>
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br />
     <br />
    <br />


<div class="publication">
            <img src="./logo/EECS.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">Towards Advanced Emotional Care: Embodied Emotional Care System for Humanoid Robots</a>
                </strong>
		  <br>
		<em><b>ICME, 2025 </b></em>
                <br> Yang Chang, Aoxing Li, Yuxuan Lin, ..., <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>.<br>

                <span class="links">
                    <a href="">PDF</a>
                    <a href="">Project Page</a>
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br />
     <br />
    <br />


        <div class="publication">
            <img src="./logo/MGR3Net.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">MGR<sup>3</sup>Net: Multi-Granularity Region Relation Representation Network for Facial Expression Recognition in Affective Robots</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Industrial Informatics, 2024 <a href="" target="_blank"></a> </b></em>
                <br>
               <b>Yan Wang</b>, Shaoqi Yan, Wei Song, Antonio Liotta, Jing Liu, Dingkang Yang, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />

	<div class="publication">
            <img src="./logo/LCGen.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D</a>
                </strong>
		  <br>
		<em><b>NeurIPS, 2024 </b></em>
                <br> Zeng Tao, Tong Yang, Junxiong Lin, Xinji Mai, Haoran Wang, Beining Wang, Enyu Zhou, <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>.<br>

                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



 <div class="publication">
            <img src="./logo/SSR_SR.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2403.05808">Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution</a>
                </strong>
		  <br>
		<em><b>ECCV, 2024</b></em>
                <br>
                Junxiong Lin, <b>Yan Wang<sup>✉</sup></b>, Zeng Tao, Boyang Wang, Qing Zhao, Haorang Wang, Xuan Tong, Xinji Mai, Yuxuan Lin, ..., Wenqiang Zhang.<br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	<br />


 <div class="publication">
            <img src="./logo/unifedemotion.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">All rivers run into the sea: Unified Modality Brain-like Emotional Central Mechanism</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2024 <a href="https://arxiv.org/abs/2407.15590" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
                Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, <b>Yan Wang<sup>✉</sup></b>, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang.<br>

                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />


 <div class="publication">
            <img src="./logo/KFE_SC.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Observe Finer To Select Better: Learning Key Frame Extraction via Semantic Coherence for Dynamic Facial Expression Recognition in the Wild </a>
                </strong>
		  <br>
		<em><b>Information Sciences, 2024 <a href="" target="_blank"></a> </b></em>
                <br>
               Shaoqi Yan, <b>Yan Wang<sup>✉</sup></b>, Xinji Mai , Zeng Tao, Wei Song , Qing Zhao, Boyang Wang, Haoran Wang, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />



 <div class="publication">
            <img src="./logo/UF_FGTG.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image</a>
                </strong>
		  <br>
		<em><b>AAAI, 2024</b></em>
                <br>
               Nailei Hei, Qianyu Guo, Zihao Wang, <b>Yan Wang<sup>✉</sup></b>, Haofen Wang<sup>✉</sup>, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />

     <br />
    <br />



    <div class="publication">
            <img src="./logo/USR.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2406.16459">Suppressing Uncertainties in Degradation Estimation for Blind Super-Resolution</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a> </b></em>
                <br>
               Junxiong Lin, Zeng Tao, Xuan Tong, Xinji Mai, Haoran Wang, Boyang Wang, <b>Yan Wang<sup>✉</sup></b>, Qing Zhao, Jiawen Yu, Yuxuan Lin, Shaoqi Yan, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2406.16459">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />




 <div class="publication">
            <img src="./logo/FD_AD.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">FD-UAD: Unsupervised Anomaly Detection Platform Based on Defect Autonomous Imaging and Enhancement</a>
                </strong>
		  <br>
		<em><b>IJCAI, 2024</b></em>
                <br>
                Yang Chang, Yuxuan Lin, Boyang Wang, Qing Zhao, <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>. <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	<br />


        <div class="publication">
            <img src="./logo/MSC-AD.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">MSC-AD: A Multi-Scene Unsupervised Anomaly Detection Dataset for Small Defect Detection of Casting Surface</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Industrial Informatics, 2023 <a href="" target="_blank"></a> </b></em>
                <br>
               Qing Zhao, <b>Yan Wang<sup>✉</sup></b>, Boyang Wang, Junxiong Lin, Shaoqi Yan, Wei Song, Antonio Liotta, Jiawen Yu, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/10376373">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />



    <div class="publication">
            <img src="./logo/Freq-HD.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Freq-HD: an Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos</a>
		</strong>
		  <br>
		<em><b>ACM MM, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading5" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
               Zeng Tao, <b>Yan Wang<sup>✉</sup></b>, Zhaoyu Chen, Boyang Wang, Shaoqi Yan, Kaixun Jiang, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611972">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />


	 <div class="publication">
            <img src="./logo/ICCV23.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">Efficient Decision-based Black-box Patch Attacks on Video Recognition</a>
                </strong>
		  <br>
		<em><b>ICCV, 2023</b></em>
                <br>
		    Kaixun Jiang, Zhaoyu Chen, Tony Huang, Jiafeng Wang, Dingkang Yang, Bo Li, <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
    <br />
    <br />




      <div class="publication">
            <img src="./logo/DSA.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Towards Decision-based Sparse Attacks on Video Recognition</a>
		</strong>
		  <br>
		<em><b>ACM MM, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading12" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
                Kaixun Jiang, Zhaoyu Chen, Xinyu Zhou, Jingyu Zhang, Lingyi Hong, JiaFeng Wang, Bo Li, <b>Yan Wang<sup>✉</sup></b>, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611828">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
     <br />


        <div class="publication">
            <img src="./logo/SW_FSCL.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Empower Smart Cities with Sampling-wise Dynamic Facial Expression Recognition via Frame-Sequence Contrastive Learning</a>
                </strong>
		  <br>
		<em><b>INSAI 2023 --> Computer Communications, 2023 </b></em>
                <br>
               Shaoqi Yan, <b>Yan Wang<sup>✉</sup></b>, Xinji Mai, Qing Zhao, Wei Song, Jun Huang, Zeng Tao, Haoran Wang, Shuyong Gao, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />

	




    <div class="publication">
            <img src="./logo/IndSR.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">A Capture to Registration Framework for Realistic Image Super-Resolution in the Industry Environment</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2023</b></em>
                <br>
                Boyang Wang, <b>Yan Wang<sup>✉</sup></b>, Qing Zhao, Junxiong Lin, ..., Shuyong Gao, Wenqiang Zhang<sup>✉</sup>
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611973

          ">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
    <br />
    <br />
    <br />

        <div class="publication">
            <img src="./logo/MNMC.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Mixed Noise-Guided Mutual Constraint Framework for Unsupervised Anomaly Detection in Smart Industries</a>
                </strong>
		  <br>
		<em><b>INSAI 2023 --> Computer Communications, 2023 </b></em>
                <br>
               Qing Zhao, <b>Yan Wang<sup>✉</sup></b>,Yuxuan Lin, Shaoqi Yan, Wei Song, Boyang Wang, Jun Huang, Yang Chang, Lizhe Qi<sup>✉</sup>, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />

	

    <div class="publication">

	    <video id="myVedio" class="publogo" autoplay="autoplay" controls="autoPlay" loop="loop" width="300 px" height="180 px" onended="this.currentTime = 0; this.play();" autoplay >
	<source src="logo/20221018_151907_Trim.mp4"></source>
      </video>
	<!--             <img src="logo/ferv39k_cvpr2022.png" class="publogo" width="300 px" height="150 px"> -->
            <p>
                <strong>
                    <a href="">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos</a>
                </strong>
		  <br>
		<em><b>CVPR, 2022</b></em>  <strong>
                <br>
                <b>Yan Wang</b>, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang, Weifeng Ge<sup>✉</sup>, Wenqiang Zhang<sup>✉</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
                    <a href="https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/FERV39k">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
      <br />

<!--    <font color="#ff0000">[Three Accept]</font> </strong>-->

 <div class="publication">
            <img src="./logo/DPCNet.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">DPCNet: Dual Path Multi-Excitation Collaborative Network for Facial Expression Representation Learning in Videos</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2022</b></em>
                <br>
                <b>Yan Wang</b>, Yixuan Sun, Wei Song, Shuyong Gao, Yiwen Huang, Zhaoyu Chen, Weifeng Ge<sup>✉</sup>, Wenqiang Zhang<sup>✉</sup>. <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />




 <div class="publication">
            <img src="./logo/BoT_MALB.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">Enhancement of Underwater Images with Statistical Model of Background Light and Optimization of Transmission Map</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Broadcasting, 2020</b></em>  <strong> <font color="#ff0000">[ESI Highly Cited Paper]</font> </strong>
                <br>
               Wei Song, <b>Yan Wang<sup>✉</sup></b>, Dongmei Huang<sup>✉</sup>, Antonio Liotta, Cristian Perra.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8957276/">PDF</a>|
                    <a href="">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/Enhancement-of-Underwater-Images-with-Statistical-Model-of-BL-and-Optimization-of-TM">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />



	  <div class="publication">
            <img src="./logo/Access_underwater_logo.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">An Experimental-based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging</a>
                </strong>
		  <br>
		<em><b>IEEE, 2019</b></em>
                <br>
               <b>Yan Wang</b>, Wei Song<sup>✉</sup>, Giancarlo Fortino, Lizhe Qi, Wenqiang Zhang, Antonio Liotta.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8782094">PDF</a>|
		    <a href="">Project Page</a>|
		    <a href="https://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
       <br />




     <div class="publication">
            <img src="./logo/PCM_logo.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">A Rapid Scene Depth Estimation Model Based on Underwater Light Attenuation Prior for Underwater Image Restoration</a>
                </strong>
		  <br>
		<em><b>PCM, 2018 <a href="" target="_blank"><font color="#ff0000">[3 Accept, Oral]</font></a></b></em>
                <br>
               Wei Song<sup>*</sup>, <b>Yan Wang<sup>*</sup></b>, Dongmei Huang, Dian Tjondronegoro.
                <br>
                <span class="links">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-00776-8_62

        ">PDF</a>|
                    <a href="">Project Page</a>
                    <a href="https://github.com/wangyanckxx/Enhancement-of-Underwater-Images-with-Statistical-Model-of-BL-and-Optimization-of-TM">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br>
	<br />
	<br />


<hr />


 <h2>


<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honors & Awards:</h2>

<ul>

<li>In 2024, Information Fusion 2024 Best Survey Award</li>
<li>In 2024, Top Ten Scientific and Technological Advances" of Fudan University (Fudan Guanghua No. 1 Humanoid Robot)</li>
<li>In 2023, Outstanding graduate of Shanghai</li>
<li>In 2022, National Scholarship for Graduate Students</li>
<li>In 2021, first-class scholarship in Fudan University</li>
<li>In 2020, Dong Scholarship in Fudan University</li>
<li>In 2020, Outstanding Graduate student of Postgraduate backbone in Fudan University</li>
<li>In 2020, Excellent graduation thesis of Shanghai Ocean University</li>
<li>One paper is recognized as  <a href="https://ieeeaccess.ieee.org/featured-articles/image_enhancement/" target="_blank"><font color="#ff0000"><strong>Featured article </strong></font></a> in IEEE.</li>
<li>In 2019, Alumni Liaison Ambassador of Shanghai Ocean University</a></li>
<li>In 2019, Outstanding graduate of Shanghai</li>
<li>In 2018, National Scholarship for Graduate Students</li>


</ul>
<br>
<hr /> 



 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Service:</h2>

	<ul>
	<li> <b>Program Committee Member/Reviewer</b>
		<ul>
<!-- 			<li>ICLR: 2019, 2020, 2021, 2022</li> -->
<!-- 			<li>ICML: 2019, 2020, 2021, 2022</li> -->
<!-- 			<li>NeurIPS: 2019, 2020, 2021</li> -->
			<li>CVPR: 2021, 2022, 2023, 2024</li>
			<li>ICCV: 2021, 2023</li>
			<li>ECCV: 2022, 2024</li>
			<li>AAAI: 2022, 2023, 2024, 2025</li>
			<li>ACM MM: 2022, 2023, 2024</li>
<!-- 			<li>IJCAI: 2020, 2021</li> -->
<!-- 			<li>KDD: 2019, 2021</li> -->
<!-- 			<li>ICDM: 2021</li> -->
<!-- 			<li>SDM: 2021</li> -->
		</ul>
	</li>

	<li> <b>Journal Reviewer</b>
		<ul>
<!-- 			<li>Nature Communications</li> -->
 			<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
            <li>International Journal of Computer Vision (IJCV)</li>
			<li>IEEE Transactions on Image Processing (TIP)</li>
 			<li>IEEE Transactions on Industrial Informatics (TII)</li>
			<li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
			<li>IEEE Transactions on Multimedia (TMM)</li>
            <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
			<li>IEEE Transactions on Broadcasting (TBC)</li>
			<li>Pattern Recognition</li>
			<li>Knowledge-Based Systems</li>


		</ul>
	</li>

</ul>

<br>


<hr />

    <center><font face="Arial">
        <br>
            © Yan Wang (王 龑) | Last updated: Apr. 2025.
			<!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
Visitor number:
<img src="https://hitwebcounter.com/counter/counter.php?page=19924415&style=0009&nbdigits=6&type=page&initCount=5798" title="Counter Widget" Alt="Visit counter For Websites"   border="0" /></a>
        </font>

		</center>

</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
