<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.png" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.png" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Yan Wang</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
<link rel="stylesheet" href="./stylesheets/styles.css">
<link rel="stylesheet" href="./stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./javascripts/analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<h7>Yan Wang </h7><br><br>
<div>
<img src="./sub_img/MyPhoto_OceanPark.jpg" border="0" width="80%"><br></div><br>


<p>
<small>üìç Handan-220, Fudan university, China</small><br>
<small>üìßyanwang19 at fudan.edu.cn</small><br>
<small>üìßyanwang9310 at 163.com</small><br><br>
<a href="https://github.com/wangyanckxx/" target="_blank">[GitHub]</a>
<a href="https://dblp.org/pid/59/2227-68.html" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl=zh-CN" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://wangyanckxx.github.io/">Homepage</a></p>
<!--<p class="view"><a href="sub_publication.html">Publications</a></p>-->
<!--<p class="view"><a href="datasets.html">Datasets</a></p>-->
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>


<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Wang Yan (ÁéãÈæë)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<!--<br><strong>2021/10 - </strong>, I am a Research Assistant Professor with the School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>-->
<!--<br><strong>2020/01 - 2021/10</strong>, I was a research fellow at the <a href="https://www.mmlab-ntu.com/"><font color="#1C86EE"> MMLab@NTU</font></a> and worked with Dr.<a href="http://personal.ie.cuhk.edu.hk/~ccloy/index.html"><font color="#1C86EE"> Chen Change Loy</font></a>,  School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>-->
<br><strong>2019/09 - 2023/01</strong>, I was a Ph.D. student in Academy for Engineering and Technology, Fudan University, China. Supervisor: <a href="http://faet.fudan.edu.cn/17/bb/c13532a137147/page.htm" target="_blank" >Prof. Wenqiang Zhang</a>. <br>
<!--<br><strong>2016/12 - 2017/12</strong>, I was a joint Ph.D. student at Research School of Engineering, Australian National University (ANU), Canberra, Australia, under the supervision of Prof. <a href="http://www.porikli.com/"><font color="#1C86EE">Fatih Porikli</font></a> (IEEE Fellow).</br>-->
<br><strong>2016/09 - 2019/07</strong>, I was a Master student in College of Information Technology (Outstanding graduate of Shanghai) Shanghai Ocean University, China. Supervisor: <a href="https://baike.baidu.com/item/%E9%BB%84%E5%86%AC%E6%A2%85/57187?fr=aladdin">Prof. Dongmei Huang </a> and <a href="https://yjs.shou.edu.cn/2020/1110/c16440a279457/page.htm" >Prof. Wei Song</a>.<br>

<hr />
</p>
 
<!--<h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hiring:</h2>-->
 
<!--<br><font color="blue">We are looking for Research Fellow, Research Assistant, and Project Officer who want to conduct-->
<!--research and develop advanced deep learning algorithms for image and video enhancement and restoration, computational imaging, and image signal processor. <a href="https://www.mmlab-ntu.com/careers.html" target="_blank"><font color="#ff0000">[Join Us]</font></a></font></p></br>-->

<!--</ul>-->
<!--<br>	-->


<!--<hr />-->
<!--</p>-->

<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>My primary reseach interests include artificial intelligence,computer vision, and image processing, Affective Computing,
   particularly in the domains of
  <li><strong>Underwater Imaging, Image Restoration and Enhancement.</strong> The purpose is to develop algorithms to process an image so that result is more suitable than original image or video for specific application. The specific research topics are</li>
  <ol type="a" start="1">
      <li>Enhancing the images captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day) in terms of pixel distribution</li>
      <li>Restoring images captured in special circumstances or devices (underwater and weak illumination) in terms of optical imaging mechanism</li>
      <li>General Image Enhancement and Restoration</li>
<!--      <li>image/depth super-resolution, image deblurring, image denosing</li>-->
  </ol>
  <li><strong>Facial Expression Recognition.</strong> The purpose is to design AI models to perceive and understand human emotion from facial images or videos. The specific research topics are</li>
  <ol type="a" start="1">
      <li>Pose/occlusion-free Facial Expression Recognition</li>
      <li>Cross-domain Facial Expression Recognition</li>
      <li>Scene-aware Dynamic Facial Expression Recognition</li>
      <li>Dataset Construction for Dynamic Facial Expression Recognition and Benchmark</li>
  </ol>

  
</ul>
<br>
<hr />
</p>
 
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recent News:</h2>
 
<ul>
<li> 2022/06 --Two paper has been accepted by <strong>ACMMM 2022</strong></li>
<li> 2022/03 --One paper has been accepted by <strong>Information Fusion</strong></li>
<li> 2022/02 --One paper has been accepted by <strong>CVPR 2022</strong></li>
<li> 2021/11 --One paper has been accepted by <strong>AAAI 2022</strong></li>
<li> 2020/09 --One paper has been accepted by <strong>IEEE Trans. Broadcasting</strong></li>

	
<!--<li> 2022/02 &#45;&#45;I am homored to be recognised as the <strong>Outstanding Reviewer</strong> of the IEEE Journal of Oceanic Engineering.</li>-->
<!--<li> 2022/01 &#45;&#45;One paper has been recognized as <strong>ESI Hot Paper</strong></li>-->
<!--<li> 2022/01 &#45;&#45;One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>-->
<!--<li> 2022/01 &#45;&#45;One papers is recognized as  <strong>Popular Documents</strong> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=6221036" target="_blank"><font color="#ff0000">(the 50 most frequently accessed documents)</font></a> in  <strong>IEEE Transactions on Cybernetics</strong></li>-->
<!--<li> 2022/01 &#45;&#45;I am nominated with honor to serve as an <strong>Associate Editor</strong> of Neurocomputing (IF: 5.719).</strong></li>-->
<!--<li> 2022/01 &#45;&#45;Two papers are recognized as  <strong>Popular Documents</strong> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"><font color="#ff0000">(the 50 most frequently accessed documents)</font></a> in  <strong>IEEE Transactions on Image Processing</strong></li>-->
<!--<li> 2022/01 &#45;&#45;I am homored to be recognised as the <strong>World‚Äôs Top 2% Scientists (2020)</strong>. It is compiled by Stanford University based on the standardized citation indicators (Table_1_Authors_singleyr_2020_wopp_extracted_202108), which is avaiable online at <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3" target="_blank"><font color="#ff0000">Mendeley Database</font></a></li>-->
<!--
<li> 2021/12 --One paper has been accepted by <strong>IEEE Transactions on Geoscience and Remote Sensing  (IF: 5.6)</strong></li>
<li> 2021/11 --One paper has been accepted by <strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (IF: 17.861)</strong></li>
<li> 2021/11 --I received the runners-up award of  <strong>Innovation in Information Technology Application and Artificial Intelligent Academic Forum for Post-doctoral Talents, Tianjin, China, 2021</strong>. </li>
<li> 2021/10 --One paper has been accepted by <strong>International Conference on 3D Vision 2021</strong> </li>
<li> 2021/09 --My supervised FYP student is awarded the <strong>2021 Global Undergraduate Awards</strong> for his entry 'Learning to See in the Dark - Low Light Image Enhancement'.  <a href="https://undergraduateawards.com/" target="_blank"><font color="#ff0000">[The Global Undergraduate Awards]</font></a> Congratulations to Sihao! So proud of you! </li>
<li> 2021/09 --I am recognized as <strong>Outstanding Reviewer</strong>, IEEE ICCV 2021</li>
<li> 2021/08 --I am nominated with honor to serve as a <strong>Senior Program Committee (Meta-Reviewers)</strong> of the AAAI 2022.</strong></li>
<li> 2021/08 --We organize a <strong>special issue</strong> on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis in IEEE Journal of Oceanic Engineering (IEEE-JOE) (SCI, IF: 3.554)</li>
<li> 2021/07 --Our Zero-DCE (Zero-DCE++) for low-light image enhancement was used in the 1st place solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition <a href="https://arxiv.org/abs/2107.00818" target="_blank"><font color="#ff0000">[1st place solution]</font></a>. Check our Zero-DCE (Zero-DCE++) <a href="https://li-chongyi.github.io/Proj_Zero-DCE++.html" target="_blank"><font color="#ff0000">[Project Page]</font></a>.</li>
<li> 2021/05 --I am recognized as <strong>Outstanding Reviewer</strong>, IEEE CVPR 2021</li>
<li> 2021/05 -- One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2021/04 -- We release the code of our Zero-DCE++ (TPAMI2021).</li>
<li> 2021/04 -- We release the code of our Ucolor (TIP2021).</li>
<li> 2021/04 -- Invited talk@<strong>SenseTime</strong> about AI Photo Enhancement. You can find the video @<strong>bilibili</strong> <a href="https://www.bilibili.com/video/BV1Gv411j7HN?p=2" target="_blank"><font color="#ff0000">[Video in Chinese]</font></a>. </li>
<li> 2021/04 -- We release our deep learning-based low-light image enhancement platform <strong>LoLi-Platform</strong>. You can find the <a href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open/" target="_blank"><font color="#ff0000">[LoLi-Platform]</font></a>. <a href="https://arxiv.org/abs/2104.10729" target="_blank"><font color="#ff0000">[Survey]</font></a>. Have fun!</li>
<li> 2021/04 -- We release the code of our Under-Display Camera Image Restoration <strong>(CVPR 2021)</strong>. You can find the <a href="https://jnjaby.github.io/projects/UDC/" target="_blank"><font color="#ff0000">[Code]</font></a>. Have fun!</li>
<li> 2021/04 -- One paper has been accepted by <strong>IEEE Transactions on Image Processing</strong> </li>
<li> 2021/04 -- 6th, NTIRE 2021 Challenge on Non-Homogeneous Image Dehazing</li>
<li> 2021/04 -- One paper has been accepted by <strong>IEEE Geoscience and Remote Sensing Letters  (IF: 3.833)</strong></li>
<li> 2021/04 -- One paper has been accepted by <strong>Applied Intelligence (IF: 3.325)</strong></li>
<li> 2021/03 -- One book chapter has been published in <strong>World Scientific</strong></li>
<li> 2021/03 -- One paper has been accepted by <strong>IEEE Transactions on Pattern Analysis and Machine Intelligence (IF: 17.861)</strong></li>
<li> 2021/03 -- One paper hits on <strong>CVPR 2021</strong></li>
<li> 2021/01 -- One paper has been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Engineering Applications of Artificial Intelligence (IF: 3.526)</strong></li>
<li> 2021/01 -- One paper has been accepted by <strong>Frontiers of Computer Science (IF: 1.940)</strong></li>
<li> 2021/01 -- I am nominated with honor to serve as an <strong>Associate Editor</strong> of the Springer Journal of Signal, Image and Video Processing (IF: 1.794).</strong></li>
<li> 2020/12 -- Two papers have been recognized as <strong>ESI Highly Cited Paper</strong></li>
<li> 2020/12 -- One paper has been accepted by <strong>Information Science</strong></li>
<li> 2020/10 -- 2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence. <strong>Excellence Award</strong></li>
<li> 2020/10 -- One paper has been accepted by <strong>IEEE Transactions on Image Processing</strong> </li>
<li> 2020/09 -- One paper has been accepted by <strong>NeurIPS</strong> </li>
<li>We released a survey about image colorization (Image Colorization: A Survey and Dataset). <a href="https://arxiv.org/pdf/2008.10774.pdf" target="_blank"><font color="#ff0000">[Arxiv Version]</font></a></a></li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy, <ud2>RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</ud2> is accepted by <strong>ECCV2020</strong>. <a href="https://li-chongyi.github.io/Proj_ECCV20" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We release the testing code of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://github.com/Li-Chongyi/Zero-DCE" target="_blank"><font color="#ff0000">[Code]</font></a>. Have fun!</li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng Zheng, and Yao Zhao, <ud2>A Parallel Down-Up Fusion Network for Salient Object Detection in Optical Remote Sensing Images</ud2> is accepted by <strong>Neurocomputing</strong>.</li>
<li>We release more details of our Zero-DCE for low-light image enhancement <strong>(CVPR 2020)</strong>. You can find the <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank"><font color="#ff0000">[Project Page]</font></a>. Have fun!</li>
<li><strong>Chongyi Li</strong>, Runmin Cong, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu, Dingwen Zhang, and Qingming Huang, <ud2>ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D salient Object Detection</ud2> is accepted by <strong>IEEE Transactions on Cybernetics</strong>.</li>
<li>We released the dataset and testing model of our TIP 2019 <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2>. Have fun! <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this dataseet or code, please cite the related paper. Thanks.</p>
<li><strong>Chongyi Li</strong>, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao, <ud2>An Underwater Image Enhancement Benchmark Dataset and Beyond</ud2> is accepted by <strong>IEEE Transactions on Image Processing</strong>. <a href="https://li-chongyi.github.io/proj_benchmark.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li> 
<li>We released our PR 2019 Underwater Scene Prior Inspired Deep Underwater Image and Video Enhancement (Datasets and Code). Have fun! <a href="https://li-chongyi.github.io/proj_underwater_image_synthesis.html" target="_blank"><font color="#ff0000">[Project page]</font></a></a></li>
<p> If you use this code, please cite the related paper. Thanks.</p>
-->
</ul>
<br>
<hr />

<!--<h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Low-Light Image Enhancement Platform: LLIE-Platform</h2>  -->
<!--  -->
<!--<ul>-->
<!--<li> Different deep models may be implemented in different platforms such as Caffe, Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different configurations, GPU versions, and hardware specifications. Such requirements are prohibitive to many researchers, especially for beginners who are new to this area and may not even have GPU resources.</font></a></li>-->
<!--<li> To resolve these problems, we develop an online platform, LLIE-Platform <a href="http://mc.nankai.edu.cn/ll" target="_blank"><font color="#ff0000">[LLIE-Platform]</font></a>. -->
<!--If you use this platform, please cite our paper "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", TPAMI, 2021.</li>-->
<!--</ul>-->
<!--<br>-->
<!--<hr />-->

<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Call for Papers:</h2>  
  
<ul>
<li><font color="#000000"> Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis, IEEE Journal of Oceanic Engineering (IEEE-JOE) (SCI, IF: 3.554) <a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 30, 2021. <a href="https://ieeeoes.org/publications/ieee-journal-of-oceanic-engineering/joe-special-issues/" target="_blue"><font color="#ff0000">[Offical Link]</font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Depth-Related Processing and Applications in Visual Systems, Multimedia Tools and Applications (SCI, IF: 2.101) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 1, 2020. <a href="https://www.springer.com/journal/11042/updates/17918156" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications in Signal Processing : Image Communication (SCI, IF: 2.814) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: July 31, 2020. <a href="https://www.journals.elsevier.com/signal-processing-image-communication/call-for-papers/theories-algorithms-and-applications" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>[Closed]
<li><font color="#000000"> <strike>Special Session in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference(APSIPA ASC) 2019.</font><a href="http://www.apsipa2019.org/#" target="_blank"><font color="#ff0000"> [Link]</font>: Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#ff0000">[CFP]</strike></font></a></a></li>[Closed]

</ul>
<br>
<hr />
-->



<div class="container">
<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Projects:</h2>
        <br>


<div class="publication">
            <img src="./logo/Review_inforfusion.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances</a>
                </strong>
		  <br>
		<em><b>Accepted for Information Fusion, 2022</b></em>
                <br>
               <b>Yan Wang</b>, Wei Song, Wei Tao, Dawei Yang, Antonio Liotta,Dawei Yang, Xinlei Li, Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, and Wenqiang Zhang.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2203.06935">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />


    <div class="publication">
            <img src="logo/ferv39k_cvpr2022.png" class="publogo" width="300 px" height="150 px">
            <p>
                <strong>
                    <a href="">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos</a>
                </strong>
		  <br>
		<em><b>CVPR, 2022</b></em>
                <br>
                <b>Yan Wang</b>, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang, Weifeng Ge, Wenqiang Zhang.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
                    <a href="https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/FERV39k">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

     <div class="publication">
            <img src="./logo/BoT_logo.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">Enhancement of Underwater Images with Statistical Model of Background Light and Optimization of Transmission Map</a>
                </strong>
		  <br>
		<em><b>IEEE Transactions on Broadcasting, 2020</b></em>
                <br>
               Wei Song, <b>Yan Wang<sup>+</sup></b>, Dongmei Huang, Antonio Liotta, Cristian Perra.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8957276/">PDF</a>|
                    <a href="">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/Enhancement-of-Underwater-Images-with-Statistical-Model-of-BL-and-Optimization-of-TM">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <img src="./logo/Access_underwater_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">An Experimental-based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging</a>
                </strong>
		  <br> 
		<em><b>IEEE ACCESS, 2019</b></em>
                <br> 
               <b>Yan Wang</b>, Wei Song, Giancarlo Fortino, Lizhe Qi, Wenqiang Zhang, Antonio Liotta.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8782094">PDF</a>|
		    <a href="">Project Page</a>|
		    <a href="https://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />




<hr />



<!--<
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Participating Fundings:</h2>

<ul>
<li>"Study on the hierarchical modeling of underwater imaging and underwater image/video clearness method", The National Natural Science Foundation of China (NSFC). (Principal Participator) 
</ul>
<br>
<hr />
-->

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honors & Awards:</h2>

<ul>


<!--<li><strong>In 2021, Second-class scholarship in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Dong Scholarship in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Outstanding Graduate student of Postgraduate backbone in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Excellent graduation thesis of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Alumni Liaison Ambassador of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Outstanding graduate of Shanghai </a></strong><br></li>-->
<!--<li><strong>In 2019, Outstanding graduate of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Aquatic Scholarship of Shanghai Ocean University</a></strong><br></li>-->
<!--<li><strong>In 2018, Second Prize in the National Postgraduate Mathematical Modeling Contest </a></strong><br></li>-->
<!--<li><strong>In 2018, National Scholarship for Graduate Students</a></strong><br></li>-->
<!--<li><strong>In 2018, First-class Academic Scholarship of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2018, Third Prize of National English Competition for College Students </a></strong><br></li>-->
<!--<li><strong>In 2017, First-class Academic Scholarship of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>From 2016 to 2019, Several First-class scholarships, Merit students and Excellent League Members </a></strong><br></li>-->

<li>In 2021, Second-class scholarship in Fudan University</li>
<li>In 2020, Dong Scholarship in Fudan University</li>
<li>In 2020, Outstanding Graduate student of Postgraduate backbone in Fudan University</li>
<li>In 2020, Excellent graduation thesis of Shanghai Ocean University</li>
<li>One paper is recognized as  <a href="https://ieeeaccess.ieee.org/featured-articles/image_enhancement/" target="_blank"><font color="#ff0000"><strong>Featured article </strong></font></a> in  IEEE Access.</li>
<li>In 2019, Alumni Liaison Ambassador of Shanghai Ocean University</a></li>
<li>In 2019, Outstanding graduate of Shanghai</li>
<li>In 2018, National Scholarship for Graduate Students</li>
<!--<li>CVPR (Outstanding Reviewer in 2021). <a href="http://cvpr2021.thecvf.com/node/184"  target="_blank">PDF</a></li>-->
<!--<li>6th, NTIRE 2021 Challenge on Non-Homogeneous Image Dehazing, 2021. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/NTIRE2021_NonHomogeneous_Dehazing_Report_compressed.pdf" target="_blank">PDF</a></li>-->
<!--<li>2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence, Tianjin, China. Excellence Award. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8D%9A%E5%90%8E%E4%BC%98%E7%A7%80%E5%A5%96.pdf">PDF</a></li>-->
<!--<li>Distinguished Dissertation Award of Beijing Society of Image and Graphics, 2018.<a href="http://www.bsig.org.cn/detail/2316">Media</a></li>-->
<!--<li>"Outstanding Graduate Student" in Tianjin University, 2018.-->
<!--<li>Bohai Securities Fellowship, 2017.</li>
<li>"Merit Student" in Tianjin University, 2017.</li> 
<li>"Advanced Individual" in the creative working, 2017.</li>
<li>China Scholarship Council (CSC) scholarships, 2016.</li>
<li>The First Class Academic Scholarship in Tianjin University, 2015, 2016.</li><embed src="https://sumanbogati.github.io/sample.pdf" type="application/pdf" />
<li>"Advanced Individual" in international exchange, 2015.</li>
<li>"Advanced Individual" in the creative working, 2015.</li>-->

</ul>
<br>
<hr /> 

<!-- <h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Co-Supervision and Teaching:</h2>-->
<!--<ul>-->
<!--<li>Co-Supervisor, M.E.: Zhexin Liang (Zhejiang University),  AI Major Project: Image and Video Enhancement, NTU, 01/2022-present</li>	-->
<!--<li>Co-Supervisor, M.E.: Yuekun Dai (Peking University),  AI Major Project: Video Depth Estimation, NTU, 08/2021-present</li>-->
<!--<li>Co-Supervisor, M.E.: Zurang Liu (Beijing University of Posts and Telecommunications), AI Major Project: Deep Image Harmonization, NTU, 01/2021-present</li>-->
<!--<li>Co-Supervisor, M.E.: Qiming Ai (University of Science and Technology of China),  AI Major Project: Deep Photo Enhancement, NTU, 01/2020-12/2020</li>-->
<!--<li>Co-Supervisor, FYP: Sihao Chen, Project: Learning to See in the Dark <a href="https://drive.google.com/file/d/12Vu-n2Kw3DV3qa6VfOVTpbyy8TGfRIQz/view?usp=sharing">[Final Report&#45;&#45;Video (A+)]</a>, NTU, 06/2020-06/2021. Sihao's project is awarded the <strong>2021 Global Undergraduate Awards</strong> for his entry 'Learning to See in the Dark - Low Light Image Enhancement'.  <a href="https://undergraduateawards.com/" target="_blank"><font color="#ff0000">[The Global Undergraduate Awards]</font></a><a href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-s-chen-sihao-(computer-science-year-4)-makes-a-historical-first-win-at-the-global-undergraduate-awards-2021" target="_blank"><font color="#ff0000">[NTU News]</font></a></li>-->
<!--<li>Co-Supervisor, M.E.: Xingshu Wang (Beijing University of Posts and Telecommunications),  Project: Reference-Based Image Super-Resolution, ANU</li>-->
<!--<li>Co-Supervisor, M.E.: Yuheng Shi (Nanjing University of Posts and Telecommunications),  Project: Hand Guesture Recognition in Adverse Environment, ANU</li>-->
<!--<li>Teaching Assistant, NTU CE6126: MSAI Advanced Computer Vision, NTU, Fall 2020 </li>-->
<!--</ul>-->
<!--<br>-->
<!--<hr /> -->

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Service:</h2>

<ul>
<!--<br><strong>Associate Editor</strong></br>-->
<!--2022/01- : Neurocomputing (IF: 5.719)</br>-->
<!--2021/01- : Springer Journal of Signal, Image and Video Processing (IF: 1.794)</br> -->

<!--<br><strong>Guest Editor</strong></br>-->
<!--IEEE Journal of Oceanic Engineering (Lead Guest Editor) (IF: 2.9)</br>-->
<!--Signal Processing: Image Communication (Management Guest Editor) (IFÔºö2.779)</br>-->
<!--Multimedia Tools and Applications (IF: 2.313)-->

<!--<br><strong>Session Chair</strong></br>-->
<!--Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) 2019-->

<!--<br><strong>Senior Program Committee (Meta-Reviewers)</strong></br>-->
<!--AAAI 2022-->

<br><strong>Reviewer</strong></br>
<li><strong>IEEE</strong>:</li>TIP, TCSVT, TMM, TIM, ACCESS, MultiMedia;

<li><strong>Elsevier</strong>:</li> Neurocomputing, PRL;

<li><strong>Springer</strong>:</li>  VC, EURASIP JIVP;

<!--<li><strong>ACM</strong>:</li> Computing Surveys-->

<!--<li><strong>IET</strong>:</li> CV-->

<!--<li><strong>SPIE</strong>:</li> JEI-->

<li><strong>Conference</strong>:</li> ACM MM2020, AAAI 2021, CVPR 2022, ECCV2022;

</ul>
<br>
<hr />    
  
<!--  -->
<!--<h2>-->
<!--<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Miscellaneous:</h2>-->

<!-- -->
<!--<ul>-->
<!--<li><a href="https://unsplash.com/"><font color="#1C86EE">Unsplash</font></a></li>-->
<!--<li><a href="https://pngtree.com/"><font color="#1C86EE">Pngtree</font></a></li>-->
<!--<li><a href="https://www.wordclouds.com/"><font color="#1C86EE">WordClouds</font></a></li>-->
<!--<li><a href="https://emojipedia.org/"><font color="#1C86EE">Emojipedia</font></a></li>-->
<!--<li><a href="https://film-grab.com/"><font color="#1C86EE">FilmGrab</font></a></li>-->
<!--<li><a href="https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1/"><font color="#1C86EE">How we write rebuttals</font></a></li>-->
<!--<li><a href="https://www.computer.org/publications/tech-news/trends/deep-learning-vs-machine-learning-whats-the-difference?source=cssocial"><font color="#1C86EE">Deep Learning vs Machine Learning: What‚Äôs the Difference</font></a></li>-->
<!--</ul>-->
<!--<br>-->


<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script> -->
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>-->




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
