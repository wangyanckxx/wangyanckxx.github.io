<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./logo/hp_logo.png" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./logo/hp_logo.png" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Yan Wang (ÁéãÈæë)</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
<link rel="stylesheet" href="./stylesheets/styles.css">
<link rel="stylesheet" href="./stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./javascripts/analytics.js"></script>
</head>
<body>
<div class="wrapper">
<header>
<!-- <h7>Yan Wang </h7><br><br>-->
<div>
<img src="./sub_img/jhz_wy.jpg" border="0" width="90%"><br></div><br>


<p>
<!--<small>üìç Handan Road-220, Shanghai </small><br>-->
<small>üìç Fudan University, China</small><br>
<small>üìßyanwang19 at fudan.edu.cn</small><br>
<small>üìßyanwang9310 at 163.com</small><br><br>
<a href="https://github.com/wangyanckxx/" target="_blank">[GitHub]</a>
<a href="https://dblp.org/pid/59/2227-68.html" target="_blank">[DBLP]</a>  <br>
<a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a> <br>
</p> <br>
<p class="view"><a href="https://wangyanckxx.github.io/">Homepage</a></p>
<!--<p class="view"><a href="sub_publication.html">Publications</a></p>-->
<p class="view"><a href="" target="_blank">Publications</a></p>
<p class="view"><a href="datasets.html" target="_blank">Datasets</a></p>
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>


<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Wang Yan (ÁéãÈæë)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<!--<br><strong>2021/10 - </strong>, I am a Research Assistant Professor with the School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore.</br>-->

	
<br><strong>2022/12 - Now</strong>, I am a Postdoctoral Fellow <strong>(Shanghai Super Postdoctoral Fellow)</strong> at the Institute of Intelligent Robotics, Fudan University. Supervisor: <a href="http://faet.fudan.edu.cn/e4/28/c23898a255016/page.htm" target="_blank" >Prof. Wenqiang Zhang</a>. <br>
<br><strong>2019/09 - 2023/01</strong>, I was a Ph.D. student <strong>(China National Scholarship, Outstanding Graduate of Shanghai)</strong>  in Academy for Engineering and Technology, Fudan University, China. Supervisor: <a href="http://faet.fudan.edu.cn/e4/28/c23898a255016/page.htm" target="_blank" >Prof. Wenqiang Zhang</a>. <br>
<!--<br><strong>2016/12 - 2017/12</strong>, I was a joint Ph.D. student at Research School of Engineering, Australian National University (ANU), Canberra, Australia, under the supervision of Prof. <a href="http://www.porikli.com/"><font color="#1C86EE">Fatih Porikli</font></a> (IEEE Fellow).</br>-->
<br><strong>2016/09 - 2019/07</strong>, I was a Master student <strong>(China National Scholarship, Outstanding Graduate of Shanghai)</strong> in College of Information Technology, Shanghai Ocean University, China. Supervisor: <a href="https://scholar.google.com/citations?user=HSn4UOIAAAAJ&hl" >Prof. Wei Song</a>.<br>

<hr />
</p>

<!--<h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hiring:</h2>-->

<!--<br><font color="blue">We are looking for Research Fellow, Research Assistant, and Project Officer who want to conduct-->
<!--research and develop advanced deep learning algorithms for image and video enhancement and restoration, computational imaging, and image signal processor. <a href="https://www.mmlab-ntu.com/careers.html" target="_blank"><font color="#ff0000">[Join Us]</font></a></font></p></br>-->

<!--</ul>-->
<!--<br>	-->


<!--<hr />-->
<!--</p>-->

<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">




   Developing cutting-edge theories and technologies in artificial intelligence and machine vision, including <font color="#000000"><strong> Intelligent Emotional Robots, Machine Vision and Enhancement </strong></font>, particularly in the domains of

          <br>
	<br />

<!--<p>-->
    <li><strong>Facial Expression Recognition and Affective Computing in Different Scenes.</strong> The purpose is to design AI models to perceive and understand human emotion from facial images, videos, text, voice or multi-modal information.
    <li><strong>Anomaly Detection in Industrial Environments.</strong> The purpose is to identify visual anomalies of equipment or products through image analysis, detect defects or failures.
    <li><strong>Image Restorationand and Enhancement, Super-resolution.</strong> The purpose is to develop algorithms to process an image so that result is more suitable than original image or video for specific application.

<!--  <li><strong>Dynamic Facial Expression Recognition.</strong> The purpose is to design AI models to perceive and understand human emotion from facial images or videos. The specific research topics are</li>-->
<!--  <ol type="a" start="1">-->
<!--      <li>Pose/occlusion-free Facial Expression Recognition</li>-->
<!--      <li>Dataset Construction for Dynamic Facial Expression Recognition and Benchmark</li>-->
<!--      <li>Key Frame Extraction for Dynamic Facial Expression Recognition</li>-->
<!--      <li>Cross-domain Dynamic Facial Expression Recognition</li>-->
<!--  </ol>-->


<!--  <li><strong>Image Restorationand and Enhancement, Super-resolution.</strong> The purpose is to develop algorithms to process an image so that result is more suitable than original image or video for specific application. The specific research topics are</li>-->
<!--  <ol type="a" start="1">-->
<!--      <li>Enhancing the images captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day) in terms of pixel distribution</li>-->
<!--      <li>Restoring images captured in special circumstances or devices (underwater and weak illumination) in terms of optical imaging mechanism</li>-->
<!--      <li>General Image Enhancement and Restoration</li>-->
<!--      <li>Super-resolution in Complex Scenes</li>-->
<!--&lt;!&ndash;      <li>image/depth super-resolution, image deblurring, image denosing</li>&ndash;&gt;-->
<!--  </ol>-->

<!--      <li><strong>Anomaly Detection in Industrial Environments.</strong> The purpose is to construct dataset and develop algorithms to detect the minor anomaly in Industrial Casting Surface. The specific research topics are</li>-->
<!--  <ol type="a" start="1">-->
<!--      <li>Unsupervised Anomaly Detection Dataset for Industrial Casting Surface Inspection</li>-->
<!--      <li>Multi-Scene Unsupervised Anomaly Detection</li>-->
<!--      <li>General Unsupervised Anomaly Detection</li>-->
<!--&lt;!&ndash;      <li>image/depth super-resolution, image deblurring, image denosing</li>&ndash;&gt;-->
<!--  </ol>-->




</ul>
<br>
<hr />
<!--</p>-->



<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>News and Olds</h2>

<ul>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-12]</span>  One paper has been accepted by <strong>AAAI 2025</strong></li>

 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-11]</span>  <strong>Congratulations.</strong> "A systematic review on affective computing: emotion models, databases, and recent advances" was selected as <strong> <font color="#ff0000">Information Fusion 2024 Best Survey Award</font> </strong></li>
	
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-10]</span>  A Survey on RGB, 3D, and Multimodal Approaches for Unsupervised Industrial Anomaly Detection can be viewed from <strong> <a href="https://arxiv.org/abs/2410.21982" target="_blank">[Arxiv]</a></strong>  <strong> <a href="https://github.com/Sunny5250/Awesome-Multi-Setting-UIAD" target="_blank">[Github]</a></strong></li>

 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-09]</span>  One paper has been accepted by <strong>NeurIPS 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-08]</span>  A Survey on Facial Expression Recognition of Static and Dynamic Emotions can be viewed from <strong> <a href="https://arxiv.org/pdf/2408.15777" target="_blank">[Arxiv]</a></strong>  <strong> <a href="https://github.com/wangyanckxx/SurveyFER" target="_blank">[Github]</a></strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-08]</span>  One paper has been accepted by <strong>Information Sciences</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-07]</span>  Two papers ( <strong> <font color="#ff0000">One Oral</font> </strong> ) have been accepted by <strong>ACMMM 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-17]</span>  One paper has been accepted by <strong>ECCV 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-04]</span>  One paper has been accepted by <strong>IJCAI 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-03]</span>  Two papers have been accepted by <strong>CVPR 2024</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-02]</span>  We will release a Multi-Scene Unsupervised Anomaly Detection Dataset <strong> MSC-AD </strong> </li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-02]</span>  We will release the<strong> IndSR </strong> dataset</li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2024-01]</span>  One paper has been accepted by <strong>IEEE Trans. on Industrial Informatics (IF: 12.3)</strong></li>
  <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  One paper has been accepted by <strong>AAAI 2024</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  Two papers have been accepted by <strong>Computer Communications (IF: 6.0)</strong></li>
 <li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-11]</span>  One paper has been accepted by <strong>IEEE Trans. on Industrial Informatics (IF: 12.3)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-10]</span>  Three papers (<strong> <font color="#ff0000">Best Paper Nomination</font> </strong>) has been accepted by <strong>INSAI 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-12]</span>  Five papers ( <strong> <font color="#ff0000">Two Oral Papers</font> </strong> ) have been accepted by <strong>ACMMM 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-07]</span>  Two papers have been accepted by <strong>ICCV 2023</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2023-03]</span>  One paper has been accepted by <strong>IEEE Trans. on CSVT (IF: 8.4)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-06]</span>  Two paper have been accepted by <strong>ACMMM 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-05]</span>  We release the <strong> FERV39k</strong> dataset</li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-03]</span>  One paper has been accepted by <strong>Information Fusion (IF: 18.6)</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2022-02]</span>  One paper has been accepted by <strong>CVPR 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2021-11]</span>  One paper has been accepted by <strong>AAAI 2022</strong></li>
<li> <span style="font-weight: 600; color: rgb(57, 45, 58);">[2020-09]</span>  One paper has been accepted by <strong>IEEE Trans. on Broadcasting</strong></li>


</ul>
<br>
<hr />

<!--<h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Low-Light Image Enhancement Platform: LLIE-Platform</h2>  -->
<!--  -->
<!--<ul>-->
<!--<li> Different deep models may be implemented in different platforms such as Caffe, Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different configurations, GPU versions, and hardware specifications. Such requirements are prohibitive to many researchers, especially for beginners who are new to this area and may not even have GPU resources.</font></a></li>-->
<!--<li> To resolve these problems, we develop an online platform, LLIE-Platform <a href="http://mc.nankai.edu.cn/ll" target="_blank"><font color="#ff0000">[LLIE-Platform]</font></a>. -->
<!--If you use this platform, please cite our paper "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", TPAMI, 2021.</li>-->
<!--</ul>-->
<!--<br>-->
<!--<hr />-->

<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Call for Papers:</h2>

<ul>
<li><font color="#000000"> Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis, IEEE Journal of Oceanic Engineering (IEEE-JOE) (SCI, IF: 3.554) <a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 30, 2021. <a href="https://ieeeoes.org/publications/ieee-journal-of-oceanic-engineering/joe-special-issues/" target="_blue"><font color="#ff0000">[Offical Link]</font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Depth-Related Processing and Applications in Visual Systems, Multimedia Tools and Applications (SCI, IF: 2.101) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: November 1, 2020. <a href="https://www.springer.com/journal/11042/updates/17918156" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>
<li><font color="#000000"> <strike>Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications in Signal Processing : Image Communication (SCI, IF: 2.814) <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#ff0000">[CFP]</font></a></a>. Submission Deadline: July 31, 2020. <a href="https://www.journals.elsevier.com/signal-processing-image-communication/call-for-papers/theories-algorithms-and-applications" target="_blue"><font color="#ff0000">[Offical Link]</strike></font></a></a></li>[Closed]
<li><font color="#000000"> <strike>Special Session in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference(APSIPA ASC) 2019.</font><a href="http://www.apsipa2019.org/#" target="_blank"><font color="#ff0000"> [Link]</font>: Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#ff0000">[CFP]</strike></font></a></a></li>[Closed]

</ul>
<br>
<hr />
-->



<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprints/Manuscripts <a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a></h2>


     <br>


 <div class="publication">
            <img src="./logo/FER_Review.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">A Survey on Facial Expression Recognition of Static and Dynamic Emotions</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>
                <b>Yan Wang</b>,... , Wenqiang Zhang<sup>‚úâ</sup>. <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2408.15777">PDF</a>|
		            <a href="https://github.com/wangyanckxx/SurveyFER">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/SurveyFER">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	


 <div class="publication">
            <img src="./logo/Hi_EF.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Hi-EF: Benchmarking for Human-interaction-based Emotion Forecasting</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>
               Haoran Wang, Xinji Mai, Zeng Tao, Yan Wang, <b>Yan Wang<sup>‚úâ</sup></b>, Jiawen Yu, Ziheng Zhou, Xuan Tong, Shaoqi Yan, Qing Zhao, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>. <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



 <div class="publication">
            <img src="./logo/Align_DFER.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2403.04294">A3lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP</a>
                </strong>
		  <br>
		<em><b>arXiv preprint, 2024</b></em>
                <br>
               Zeng Tao, <b>Yan Wang<sup>‚úâ</sup></b>, Junxiong Lin, Haoran Wang, Xinji Mai,... , Wenqiang Zhang<sup>‚úâ</sup> <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



    <hr />

<div class="container">

    <h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Publications (by reseach interest) <a href="https://scholar.google.com/citations?user=RQSDgFkAAAAJ&hl" target="_blank">[Google Scholar]</a></h2>


<!--   < color="#ff0000"><strong>(* indicates equal contribution, and ‚úâ indicates corresponding author) </strong><-->
<!--<br>-->

<font color="#000000"><strong>(<sup>‚úâ</sup> indicates corresponding author, and <sup>*</sup> indicates equal contribution) </strong></font>

    <br>
	<br />


<!--<b>ICCV, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading5" target="_blank"><font color="#ff0000">[Oral]</font></a></b>-->
<!--<hr />-->
    <br>
<h3> <font color="#930618">Intelligent Emotional Robots</font></h3>
 <br>



	
<div class="publication">
            <img src="./logo/Review_inforfusion.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">A Systematic Review on Affective Computing: Emotion Models, Databases, and Recent Advances</a>
                </strong>
		  <br>
		<em><b>Information Fusion, 2022</b></em>  <strong> <font color="#ff0000">[ESI Highly Cited Paper]</font> </strong>
                <br>
               <b>Yan Wang</b>, Wei Song, Wei Tao, Dawei Yang, Antonio Liotta,Dawei Yang, Xinlei Li, Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2203.06935">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />


<div class="publication">
            <img src="./logo/OUS_aaai2025.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">OUS: Bridging Scene Context and Facial Features to Overcome the Rigid Cognitive Problem</a>
                </strong>
		  <br>
		<em><b>AAAI, 2025 </b></em>
                <br> Xinji Mai, Haoran Wang, Zeng Tao, Junxiong Lin, Shaoqi Yan, <b>Yan Wang<sup>‚úâ</sup></b>, Jiawen Yu, Xuan Tong, Yating Li, Wenqiang Zhang<sup>‚úâ</sup>.<br>

                <span class="links">
                    <a href="https://arxiv.org/pdf/2405.18769">PDF</a>
                    <a href="">Project Page</a>
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br />
     <br />
    <br />

	
        <div class="publication">
            <img src="./logo/MGR3Net.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">MGR<sup>3</sup>Net: Multi-Granularity Region Relation Representation Network for Facial Expression Recognition in Affective Robots</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Industrial Informatics, 2024 <a href="" target="_blank"></a> </b></em>
                <br>
               <b>Yan Wang</b>, Shaoqi Yan, Wei Song, Antonio Liotta, Jing Liu, Dingkang Yang, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />

	<div class="publication">
            <img src="./logo/LCGen.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D</a>
                </strong>
		  <br>
		<em><b>NeurIPS, 2024 </b></em>
                <br> Zeng Tao, Tong Yang, Junxiong Lin, Xinji Mai, Haoran Wang, Beining Wang, Enyu Zhou, <b>Yan Wang<sup>‚úâ</sup></b>, Wenqiang Zhang<sup>‚úâ</sup>.<br>

                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />

 <div class="publication">
            <img src="./logo/KFE_SC.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Observe Finer To Select Better: Learning Key Frame Extraction via Semantic Coherence for Dynamic Facial Expression Recognition in the Wild </a>
                </strong>
		  <br>
		<em><b>Information Sciences, 2024 <a href="" target="_blank"></a> </b></em>
                <br>
               Shaoqi Yan, <b>Yan Wang<sup>‚úâ</sup></b>, Xinji Mai , Zeng Tao, Wei Song , Qing Zhao, Boyang Wang, Haoran Wang, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />

 <div class="publication">
            <img src="./logo/unifedemotion.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">All rivers run into the sea: Unified Modality Brain-like Emotional Central Mechanism</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2024 <a href="https://arxiv.org/abs/2407.15590" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
                Xinji Mai, Junxiong Lin, Haoran Wang, Zeng Tao, <b>Yan Wang<sup>‚úâ</sup></b>, Shaoqi Yan, Xuan Tong, Jiawen Yu, Boyang Wang, Ziheng Zhou, Qing Zhao, Shuyong Gao, Wenqiang Zhang.<br>

                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />



    <div class="publication">
            <img src="./logo/Freq-HD.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Freq-HD: an Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos</a>
		</strong>
		  <br>
		<em><b>ACM MM, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading5" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
               Zeng Tao, <b>Yan Wang<sup>‚úâ</sup></b>, Zhaoyu Chen, Boyang Wang, Shaoqi Yan, Kaixun Jiang, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611972">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />

      <div class="publication">
            <img src="./logo/DSA.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Towards Decision-based Sparse Attacks on Video Recognition</a>
		</strong>
		  <br>
		<em><b>ACM MM, 2023 <a href="https://dl.acm.org/doi/proceedings/10.1145/3581783#heading12" target="_blank"><font color="#ff0000">[Oral]</font></a> </b></em>
                <br>
                Kaixun Jiang, Zhaoyu Chen, Xinyu Zhou, Jingyu Zhang, Lingyi Hong, JiaFeng Wang, Bo Li, <b>Yan Wang<sup>‚úâ</sup></b>, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611828">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
     <br />


	 <div class="publication">
            <img src="./logo/ICCV23.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">Efficient Decision-based Black-box Patch Attacks on Video Recognition</a>
                </strong>
		  <br>
		<em><b>ICCV, 2023</b></em>
                <br>
		    Kaixun Jiang, Zhaoyu Chen, Tony Huang, Jiafeng Wang, Dingkang Yang, Bo Li, <b>Yan Wang<sup>‚úâ</sup></b>, Wenqiang Zhang<sup>‚úâ</sup>
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
    <br />
    <br />

        <div class="publication">
            <img src="./logo/SW_FSCL.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">Empower Smart Cities with Sampling-wise Dynamic Facial Expression Recognition via Frame-Sequence Contrastive Learning</a>
                </strong>
		  <br>
		<em><b>INSAI 2023 --> Computer Communications, 2023 </b></em>
                <br>
               Shaoqi Yan, <b>Yan Wang<sup>‚úâ</sup></b>, Xinji Mai, Qing Zhao, Wei Song, Jun Huang, Zeng Tao, Haoran Wang, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />


    <div class="publication">

	    <video id="myVedio" class="publogo" autoplay="autoplay" controls="autoPlay" loop="loop" width="300 px" height="180 px" onended="this.currentTime = 0; this.play();" autoplay >
	<source src="logo/20221018_151907_Trim.mp4"></source>
      </video>
	<!--             <img src="logo/ferv39k_cvpr2022.png" class="publogo" width="300 px" height="150 px"> -->
            <p>
                <strong>
                    <a href="">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos</a>
                </strong>
		  <br>
		<em><b>CVPR, 2022</b></em>  <strong> <font color="#ff0000">[Three Accept]</font> </strong>
                <br>
                <b>Yan Wang</b>, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang, Weifeng Ge<sup>‚úâ</sup>, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
                    <a href="https://wangyanckxx.github.io/Proj_CVPR2022_FERV39k.html">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/FERV39k">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
      <br />

 <div class="publication">
            <img src="./logo/DPCNet.png" class="publogo" width="300 px" height="180 px">
            <p>
                <strong>
                    <a href="">DPCNet: Dual Path Multi-Excitation Collaborative Network for Facial Expression Representation Learning in Videos</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2022</b></em>
                <br>
                <b>Yan Wang</b>, Yixuan Sun, Wei Song, Shuyong Gao, Yiwen Huang, Zhaoyu Chen, Weifeng Ge<sup>‚úâ</sup>, Wenqiang Zhang<sup>‚úâ</sup>. <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
    <br />

    <div class="publication">
            <img src="./logo/UF_FGTG.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image</a>
                </strong>
		  <br>
		<em><b>AAAI, 2024</b></em>
                <br>
               Nailei Hei, Qianyu Guo, Zihao Wang, <b>Yan Wang<sup>‚úâ</sup></b>, Haofen Wang<sup>‚úâ</sup>, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />

     <br />
    <br />

	



<!--   <hr />-->
   <br>
<h3> <font color="#930618">Machine Vision and Enhancement</font></h3>
 <br>


        <div class="publication">
            <img src="./logo/MSC-AD.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="">MSC-AD: A Multi-Scene Unsupervised Anomaly Detection Dataset for Small Defect Detection of Casting Surface</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Industrial Informatics, 2023 <a href="" target="_blank"></a> </b></em>
                <br>
               Qing Zhao, <b>Yan Wang<sup>‚úâ</sup></b>, Boyang Wang, Junxiong Lin, Shaoqi Yan, Wei Song, Antonio Liotta, Jiawen Yu, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/10376373">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />





 <div class="publication">
            <img src="./logo/SSR_SR.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2403.05808">Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution</a>
                </strong>
		  <br>
		<em><b>ECCV, 2024</b></em>
                <br>
                Junxiong Lin, <b>Yan Wang<sup>‚úâ</sup></b>, Zeng Tao, Boyang Wang, Qing Zhao, Haorang Wang, Xuan Tong, Xinji Mai, Yuxuan Lin, ..., Wenqiang Zhang.<br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	<br />



    <div class="publication">
            <img src="./logo/USR.png" class="publogo" width="300 px" height="230 px">
            <p>
                <strong>
                    <a href="https://arxiv.org/abs/2406.16459">Suppressing Uncertainties in Degradation Estimation for Blind Super-Resolution</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a> </b></em>
                <br>
               Junxiong Lin, Zeng Tao, Xuan Tong, Xinji Mai, Haoran Wang, Boyang Wang, <b>Yan Wang<sup>‚úâ</sup></b>, Qing Zhao, Jiawen Yu, Yuxuan Lin, Shaoqi Yan, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2406.16459">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />
    <br />



 <div class="publication">
            <img src="./logo/FD_AD.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">FD-UAD: Unsupervised Anomaly Detection Platform Based on Defect Autonomous Imaging and Enhancement</a>
                </strong>
		  <br>
		<em><b>IJCAI, 2024</b></em>
                <br>
                Yang Chang, Yuxuan Lin, Boyang Wang, Qing Zhao, <b>Yan Wang<sup>‚úâ</sup></b>, Wenqiang Zhang<sup>‚úâ</sup>. <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />
	<br />


    <div class="publication">
            <img src="./logo/IndSR.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">A Capture to Registration Framework for Realistic Image Super-Resolution in the Industry Environment</a>
                </strong>
		  <br>
		<em><b>ACM MM, 2023</b></em>
                <br>
                Boyang Wang, <b>Yan Wang<sup>‚úâ</sup></b>, Qing Zhao, Junxiong Lin, Zeng Tao, Pinxue Guo, Zhaoyu Chen, Kaixun Jiang, Shaoqi Yan, Shuyong Gao, Wenqiang Zhang<sup>‚úâ</sup>
                <br>
                <span class="links">
                    <a href="https://doi.org/10.1145/3581783.3611973

          ">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
    <br />
    <br />
    <br />

        <div class="publication">
            <img src="./logo/MNMC.png" class="publogo" width="300 px" height="200 px">
            <p>
                <strong>
                    <a href="">Mixed Noise-Guided Mutual Constraint Framework for Unsupervised Anomaly Detection in Smart Industries</a>
                </strong>
		  <br>
		<em><b>INSAI 2023 --> Computer Communications, 2023 </b></em>
                <br>
               Qing Zhao, <b>Yan Wang<sup>‚úâ</sup></b>,Yuxuan Lin, Shaoqi Yan, Wei Song, Boyang Wang, Jun Huang, Yang Chang, Lizhe Qi<sup>‚úâ</sup>, Wenqiang Zhang<sup>‚úâ</sup>.
                <br>
                <span class="links">
                    <a href="">PDF</a>|
		            <a href="">Project Page</a>|
                    <a href="">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
    <br />
     <br />


 <div class="publication">
            <img src="./logo/BoT_MALB.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">Enhancement of Underwater Images with Statistical Model of Background Light and Optimization of Transmission Map</a>
                </strong>
		  <br>
		<em><b>IEEE Trans. on Broadcasting, 2020</b></em>  <strong> <font color="#ff0000">[ESI Highly Cited Paper]</font> </strong>
                <br>
               Wei Song, <b>Yan Wang<sup>‚úâ</sup></b>, Dongmei Huang<sup>‚úâ</sup>, Antonio Liotta, Cristian Perra.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8957276/">PDF</a>|
                    <a href="">Project Page</a>|
                    <a href="https://github.com/wangyanckxx/Enhancement-of-Underwater-Images-with-Statistical-Model-of-BL-and-Optimization-of-TM">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	<br />



	  <div class="publication">
            <img src="./logo/Access_underwater_logo.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">An Experimental-based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging</a>
                </strong>
		  <br>
		<em><b>IEEE, 2019</b></em>
                <br>
               <b>Yan Wang</b>, Wei Song<sup>‚úâ</sup>, Giancarlo Fortino, Lizhe Qi, Wenqiang Zhang, Antonio Liotta.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8782094">PDF</a>|
		    <a href="">Project Page</a>|
		    <a href="https://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
       <br />




     <div class="publication">
            <img src="./logo/PCM_logo.png" class="publogo" width="300 px">
            <p>
                <strong>
                    <a href="">A Rapid Scene Depth Estimation Model Based on Underwater Light Attenuation Prior for Underwater Image Restoration</a>
                </strong>
		  <br>
		<em><b>PCM, 2018 <a href="" target="_blank"><font color="#ff0000">[3 Accept, Oral]</font></a></b></em>
                <br>
               Wei Song<sup>*</sup>, <b>Yan Wang<sup>*</sup></b>, Dongmei Huang, Dian Tjondronegoro.
                <br>
                <span class="links">
                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-00776-8_62

        ">PDF</a>|
                    <a href="">Project Page</a>
                    <a href="https://github.com/wangyanckxx/Enhancement-of-Underwater-Images-with-Statistical-Model-of-BL-and-Optimization-of-TM">Github</a>
                </span>
            </p>
          </div>
          <br>
          <br>
     <br>
	<br />
	<br />




<hr />



<!--<
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Participating Fundings:</h2>

<ul>
<li>"Study on the hierarchical modeling of underwater imaging and underwater image/video clearness method", The National Natural Science Foundation of China (NSFC). (Principal Participator)
</ul>
<br>
<hr />
-->

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Honors & Awards:</h2>

<ul>


<!--<li><strong>In 2021, Second-class scholarship in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Dong Scholarship in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Outstanding Graduate student of Postgraduate backbone in Fudan University </a></strong><br></li>-->
<!--<li><strong>In 2020, Excellent graduation thesis of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Alumni Liaison Ambassador of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Outstanding graduate of Shanghai </a></strong><br></li>-->
<!--<li><strong>In 2019, Outstanding graduate of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2019, Aquatic Scholarship of Shanghai Ocean University</a></strong><br></li>-->
<!--<li><strong>In 2018, Second Prize in the National Postgraduate Mathematical Modeling Contest </a></strong><br></li>-->
<!--<li><strong>In 2018, National Scholarship for Graduate Students</a></strong><br></li>-->
<!--<li><strong>In 2018, First-class Academic Scholarship of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>In 2018, Third Prize of National English Competition for College Students </a></strong><br></li>-->
<!--<li><strong>In 2017, First-class Academic Scholarship of Shanghai Ocean University </a></strong><br></li>-->
<!--<li><strong>From 2016 to 2019, Several First-class scholarships, Merit students and Excellent League Members </a></strong><br></li>-->
<!--<li>In 2023, Outstanding graduate of Shanghai</li>-->
<li>In 2023, Outstanding graduate of Shanghai</li>
<li>In 2022, National Scholarship for Graduate Students</li>
<li>In 2021, first-class scholarship in Fudan University</li>
<li>In 2020, Dong Scholarship in Fudan University</li>
<li>In 2020, Outstanding Graduate student of Postgraduate backbone in Fudan University</li>
<li>In 2020, Excellent graduation thesis of Shanghai Ocean University</li>
<li>One paper is recognized as  <a href="https://ieeeaccess.ieee.org/featured-articles/image_enhancement/" target="_blank"><font color="#ff0000"><strong>Featured article </strong></font></a> in IEEE.</li>
<li>In 2019, Alumni Liaison Ambassador of Shanghai Ocean University</a></li>
<li>In 2019, Outstanding graduate of Shanghai</li>
<li>In 2018, National Scholarship for Graduate Students</li>
<!--<li>CVPR (Outstanding Reviewer in 2021). <a href="http://cvpr2021.thecvf.com/node/184"  target="_blank">PDF</a></li>-->
<!--<li>6th, NTIRE 2021 Challenge on Non-Homogeneous Image Dehazing, 2021. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/NTIRE2021_NonHomogeneous_Dehazing_Report_compressed.pdf" target="_blank">PDF</a></li>-->
<!--<li>2020 National Postdoctoral Forum on the Development and Application of Artificial Intelligence, Tianjin, China. Excellence Award. <a href="https://github.com/Li-Chongyi/PAPERS/blob/master/%E5%8D%9A%E5%90%8E%E4%BC%98%E7%A7%80%E5%A5%96.pdf">PDF</a></li>-->
<!--<li>Distinguished Dissertation Award of Beijing Society of Image and Graphics, 2018.<a href="http://www.bsig.org.cn/detail/2316">Media</a></li>-->
<!--<li>"Outstanding Graduate Student" in Tianjin University, 2018.-->
<!--<li>Bohai Securities Fellowship, 2017.</li>
<li>"Merit Student" in Tianjin University, 2017.</li> 
<li>"Advanced Individual" in the creative working, 2017.</li>
<li>China Scholarship Council (CSC) scholarships, 2016.</li>
<li>The First Class Academic Scholarship in Tianjin University, 2015, 2016.</li><embed src="https://sumanbogati.github.io/sample.pdf" type="application/pdf" />
<li>"Advanced Individual" in international exchange, 2015.</li>
<li>"Advanced Individual" in the creative working, 2015.</li>-->

</ul>
<br>
<hr /> 

<!-- <h2>-->
<!--<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Co-Supervision and Teaching:</h2>-->
<!--<ul>-->
<!--<li>Co-Supervisor, M.E.: Zhexin Liang (Zhejiang University),  AI Major Project: Image and Video Enhancement, NTU, 01/2022-present</li>	-->
<!--<li>Co-Supervisor, M.E.: Yuekun Dai (Peking University),  AI Major Project: Video Depth Estimation, NTU, 08/2021-present</li>-->
<!--<li>Co-Supervisor, M.E.: Zurang Liu (Beijing University of Posts and Telecommunications), AI Major Project: Deep Image Harmonization, NTU, 01/2021-present</li>-->
<!--<li>Co-Supervisor, M.E.: Qiming Ai (University of Science and Technology of China),  AI Major Project: Deep Photo Enhancement, NTU, 01/2020-12/2020</li>-->
<!--<li>Co-Supervisor, FYP: Sihao Chen, Project: Learning to See in the Dark <a href="https://drive.google.com/file/d/12Vu-n2Kw3DV3qa6VfOVTpbyy8TGfRIQz/view?usp=sharing">[Final Report&#45;&#45;Video (A+)]</a>, NTU, 06/2020-06/2021. Sihao's project is awarded the <strong>2021 Global Undergraduate Awards</strong> for his entry 'Learning to See in the Dark - Low Light Image Enhancement'.  <a href="https://undergraduateawards.com/" target="_blank"><font color="#ff0000">[The Global Undergraduate Awards]</font></a><a href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-s-chen-sihao-(computer-science-year-4)-makes-a-historical-first-win-at-the-global-undergraduate-awards-2021" target="_blank"><font color="#ff0000">[NTU News]</font></a></li>-->
<!--<li>Co-Supervisor, M.E.: Xingshu Wang (Beijing University of Posts and Telecommunications),  Project: Reference-Based Image Super-Resolution, ANU</li>-->
<!--<li>Co-Supervisor, M.E.: Yuheng Shi (Nanjing University of Posts and Telecommunications),  Project: Hand Guesture Recognition in Adverse Environment, ANU</li>-->
<!--<li>Teaching Assistant, NTU CE6126: MSAI Advanced Computer Vision, NTU, Fall 2020 </li>-->
<!--</ul>-->
<!--<br>-->
<!--<hr /> -->

 <h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Professional Service:</h2>

	<ul>
	<li> <b>Program Committee Member/Reviewer</b>
		<ul>
<!-- 			<li>ICLR: 2019, 2020, 2021, 2022</li> -->
<!-- 			<li>ICML: 2019, 2020, 2021, 2022</li> -->
<!-- 			<li>NeurIPS: 2019, 2020, 2021</li> -->
			<li>CVPR: 2021, 2022, 2023, 2024</li>
			<li>ICCV: 2021, 2023</li>
			<li>ECCV: 2022, 2024</li>
			<li>AAAI: 2022, 2023, 2024, 2025</li>
			<li>ACM MM: 2022, 2023, 2024</li>
<!-- 			<li>IJCAI: 2020, 2021</li> -->
<!-- 			<li>KDD: 2019, 2021</li> -->
<!-- 			<li>ICDM: 2021</li> -->
<!-- 			<li>SDM: 2021</li> -->
		</ul>
	</li>

	<li> <b>Journal Reviewer</b>
		<ul>
<!-- 			<li>Nature Communications</li> -->
 			<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
            <li>International Journal of Computer Vision (IJCV)</li>
			<li>IEEE Transactions on Image Processing (TIP)</li>
 			<li>IEEE Transactions on Industrial Informatics (TII)</li>
			<li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
			<li>IEEE Transactions on Multimedia (TMM)</li>
			<li>IEEE Transactions on Broadcasting (TBC)</li>
<!--			<li>IEEE Access</li>-->

<!-- 			<li>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</li> -->
<!-- 			<li>Knowledge and Information Systems (KAIS)</li> -->
			<li>Pattern Recognition</li>
			<li>Knowledge-Based Systems</li>

<!-- 			<li>IEEE Robotics and Automation Letters (RA-L)</li> -->
<!-- 			<li>Pattern Recognition Letters</li> -->
		</ul>
	</li>

</ul>

<br>


<!--<hr />    -->
  
<!--  -->
<!--<h2>-->
<!--<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Miscellaneous:</h2>-->

<!-- -->
<!--<ul>-->
<!--<li><a href="https://unsplash.com/"><font color="#1C86EE">Unsplash</font></a></li>-->
<!--<li><a href="https://pngtree.com/"><font color="#1C86EE">Pngtree</font></a></li>-->
<!--<li><a href="https://www.wordclouds.com/"><font color="#1C86EE">WordClouds</font></a></li>-->
<!--<li><a href="https://emojipedia.org/"><font color="#1C86EE">Emojipedia</font></a></li>-->
<!--<li><a href="https://film-grab.com/"><font color="#1C86EE">FilmGrab</font></a></li>-->
<!--<li><a href="https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1/"><font color="#1C86EE">How we write rebuttals</font></a></li>-->
<!--<li><a href="https://www.computer.org/publications/tech-news/trends/deep-learning-vs-machine-learning-whats-the-difference?source=cssocial"><font color="#1C86EE">Deep Learning vs Machine Learning: What‚Äôs the Difference</font></a></li>-->
<!--</ul>-->
<!--<br>-->


<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script> -->
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>-->




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
